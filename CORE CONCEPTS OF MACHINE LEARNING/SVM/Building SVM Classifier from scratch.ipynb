{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPM62CUBfcj3HckJ24qOJI/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**SVM Classifier**"],"metadata":{"id":"iUWHMWM3h5cY"}},{"cell_type":"markdown","source":["Equation of the Hyperplane:\n","\n","**y = wx - b**"],"metadata":{"id":"vyFufl7lh9Gx"}},{"cell_type":"markdown","source":["**Gradient Descent:**\n","\n","Gradient Descent is an optimization algorithm used for minimizing the loss function in various machine learning algorithms. It is used for updating the parameters of the learning model.\n","\n","w  =  w - α*dw\n","\n","b  =  b - α*db"],"metadata":{"id":"vQYDj-EJh_J7"}},{"cell_type":"markdown","source":["**Learning Rate:**\n","\n","Learning rate is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function."],"metadata":{"id":"FM85HGaLiB2W"}},{"cell_type":"markdown","source":["Importing the Dependencies"],"metadata":{"id":"zwOj7mh3iEW2"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"WvgrKK4JhRz6","executionInfo":{"status":"ok","timestamp":1767622872691,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shriram N","userId":"17850568413739029451"}}},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","source":["SUPPORT VECTOR MACHINE CLASSIFIER"],"metadata":{"id":"mgq0NztRiJHm"}},{"cell_type":"code","source":["class SVM_classifier():\n","\n","  # initiating the no. of parameters\n","  def __init__(self, learning_rate, no_of_iterations, lambda_parameter):\n","\n","    self.learning_rate = learning_rate\n","    self.no_of_iterations = no_of_iterations\n","    self.lambda_parameter = lambda_parameter\n","\n","  # fitting the dataset to SVM classifier\n","  def fit(self, X, Y):\n","\n","    # m ---> no. of data points ----> no. of rows\n","    # n ---> no. of input features ----> no. opf columns\n","    self.m, self.n = X.shape\n","\n","    # initializing the weight value and bias value\n","    self.w = np.zeros(self.n)\n","    self.b = 0\n","    self.X = X\n","    self.Y = Y\n","\n","    # implementing gradient descent algorithm for optimization\n","    for i in range(self.no_of_iterations):\n","      self.update_weights()\n","\n","  # function for updating the weight and bias\n","  def update_weights(self, ):\n","\n","    y_label = np.where(self.Y <= 0, -1, 1)\n","    for index, x_i in enumerate(self.X):\n","\n","      condition = y_label[index] * (np.dot(x_i, self.w) - self.b) >= 1\n","\n","      if (condition == True):\n","        dw = 2*self.lambda_parameter*self.w\n","        db = 0\n","      else:\n","        dw = 2*self.lambda_parameter*self.w - np.dot(x_i, y_label[index])\n","        db = y_label[index]\n","\n","      self.w = self.w - self.learning_rate*dw\n","      self.b = self.b - self.learning_rate*db\n","\n","  # predict the label for a given input label\n","  def predict(self, X):\n","\n","    output = np.dot(X, self.w) - self.b\n","    predicted_labels = np.sign(output)\n","    y_hat = np.where(predicted_labels <= -1, 0, 1)\n","    return y_hat"],"metadata":{"id":"S_mXaCy8iHyi","executionInfo":{"status":"ok","timestamp":1767628243960,"user_tz":-330,"elapsed":11,"user":{"displayName":"Shriram N","userId":"17850568413739029451"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["model = SVM_classifier(learning_rate=0.001, no_of_iterations=1000, lambda_parameter=0.01)"],"metadata":{"id":"R8EUWD14n5Gf","executionInfo":{"status":"ok","timestamp":1767628246595,"user_tz":-330,"elapsed":5,"user":{"displayName":"Shriram N","userId":"17850568413739029451"}}},"execution_count":3,"outputs":[]}]}